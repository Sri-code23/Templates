Got it! You want to run Open WebUI directly with Ollama on port 11434 using Docker, without touching Node.js or another port like 3000. Let’s keep it simple!

I’ll guide you step by step to run Open WebUI on Ollama’s port (11434) with Docker.


---

Step 1: Install Docker and Ollama in WSL

1. Update packages:



sudo apt update && sudo apt upgrade -y

2. Install Docker:



sudo apt install docker.io -y

3. Enable Docker and start the service:



sudo systemctl enable docker
sudo systemctl start docker

4. Install Ollama:



curl -fsSL https://ollama.com/install.sh | sh

5. Start the Ollama server:



ollama serve

6. Pull the DeepSeek model:



ollama pull deepseek


---

Step 2: Run Open WebUI with Docker

1. Run Open WebUI container (bind to Ollama’s port):



sudo docker run -d \
  -p 11434:11434 \
  -e OLLAMA_API_BASE_URL=http://localhost:11434 \
  --name openwebui \
  ghcr.io/open-webui/open-webui:main

Here’s what each part of the command does:

-p 11434:11434: Maps port 11434 (Ollama’s default port) to the same port in the container.

-e OLLAMA_API_BASE_URL: Sets the WebUI to use Ollama’s API.

--name openwebui: Names the container openwebui.

The Docker image comes from GitHub Container Registry (GHCR).


2. Verify the container is running:



docker ps

3. Check the logs (optional):



docker logs openwebui


---

Step 3: Access the Web Interface

1. Open your browser and go to:



http://localhost:11434

2. Test DeepSeek:



Type a message, and DeepSeek will respond through Open WebUI.



---

Step 4: Manage Your Setup

Stop the container:


docker stop openwebui

Restart the container:


docker start openwebui

Remove the container:


docker rm -f openwebui


---

That’s It!

Now you’re running Open WebUI directly on Ollama’s port (11434), with just Docker — no Node.js, no extra ports. Let me know if you want any tweaks or optimizations!